\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage[framemethod=tikz]{mdframed}
\geometry{margin=1in}

% Eliminar sangría de párrafos
\setlength{\parindent}{0pt}

\title{Procesos Estocásticos: Seccion 1}
\author{Alejandro Daniel José Gómez Flórez}
\date{}

% Macro para teoremas
\newmdenv[
    backgroundcolor=green!5,
    linecolor=green!50,
    linewidth=1.5pt,
    roundcorner=5pt,
    innertopmargin=8pt,
    innerbottommargin=8pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    leftmargin=0pt,
    rightmargin=0pt
]{teoremabox}

\newcommand{\teorema}[1]{%
\begin{teoremabox}
\textbf{Teorema}: #1
\end{teoremabox}
}

\begin{document}

\maketitle

\section{Demostraciones}


\teorema{
\begin{equation*}
P^{(n)} = P^{(m)} \cdot P^{(n-m)}, \quad \text{en particular } P^{(n)} = P \cdot P \cdots P \ (\text{$n$ veces}) = P^n.
\end{equation*}}

\textbf{Demostración:}

Sea $P_{ij}^{(n)} = P(X_n = j \mid X_0 = i)$. Para $0 \leq m \leq n$:

\begin{align*}
P_{ij}^{(n)} &= P(X_n = j \mid X_0 = i) \\
&= \sum_{k \in S} P(X_n = j, X_m = k \mid X_0 = i) \quad \text{(partición de la probabilidad total respecto a $X_m$)} \\
&= \sum_{k \in S} P(X_n = j \mid X_m = k, X_0 = i) \cdot P(X_m = k \mid X_0 = i) \\
&= \sum_{k \in S} P(X_n = j \mid X_m = k) \cdot P(X_m = k \mid X_0 = i) \quad \text{(propiedad de Markov)} \\
&= \sum_{k \in S} P_{kj}^{(n-m)} \cdot P_{ik}^{(m)} \\
&= \sum_{k \in S} P_{ik}^{(m)} P_{kj}^{(n-m)} = [P^{(m)} \cdot P^{(n-m)}]_{ij}
\end{align*}

Por tanto $P^{(n)} = P^{(m)} \cdot P^{(n-m)}$.

\qed


\teorema{Para una cadena de Markov con matriz de transición $P = (P_{ij})$:
\begin{itemize}
    \item $\sum_{n=0}^{\infty} P_{ii}^{(n)} = \infty$ si, y solo si el estado $i$ es recurrente
    \item $\sum_{n=0}^{\infty} P_{ii}^{(n)} < \infty$ si, y solo si el estado $i$ es transitorio
\end{itemize}}

\textbf{Demostración:}

Sea $N_i = \sum_{n=0}^{\infty} \mathbf{1}_{\{X_n = i\}}$ el número de visitas al estado $i$ y $\mathbf{1}_{\{X_n = i\}}$ la función indicadora de que el estado $X_n$ es $i$. \\
Sea $f^{(n)}_{ii} = P(X_n = i, X_{n-1} \neq i, \ldots, X_0 \neq i)$ la probabilidad retornar a $i$ en $n$ pasos.\\
Sea $f_{i} = \sum_{n=0}^{\infty} f^{n}_{ii}$ la probabilidad regresar al estado $i$ eventualmente.

El número de visitas $N_i$ sigue:
\begin{itemize}
\item Con probabilidad $(1-f_{i})$: exactamente 1 visita (no regresa)
\item Con probabilidad $f_{i}(1-f_{i})$: exactamente 2 visitas (regresa una vez)
\item Con probabilidad $f_{i}^2(1-f_{i})$: exactamente 3 visitas (regresa dos veces)
\item $\vdots$
\end{itemize}

Por tanto:
\begin{align*}
\mathbb{E}_i[N_i] &= 1(1-f_i) + f_i(1- f_i) + f^2_i(1-f_i) + \cdots \\
&= \frac{1}{1-f_{i}} < \infty \quad (\text{serie geométrica})
\end{align*}

Como $\mathbb{E}_i[N_i] = \sum_{n=0}^{\infty} P_{ii}^{(n)}$, tenemos:
\begin{itemize}
\item Si $i$ es recurrente: $f_i = 1 \Rightarrow \mathbb{E}_i[N_i] = \infty \Rightarrow \sum_{n=0}^{\infty} P_{ii}^{(n)} = \infty$
\item Si $i$ es transitorio: $f_i < 1 \Rightarrow \mathbb{E}_i[N_i] = \frac{1}{1-f_i} < \infty \Rightarrow \sum_{n=0}^{\infty} P_{ii}^{(n)} < \infty$
\end{itemize}

\qed

\teorema{Si la cadena de Markov es irreducible y sus estados son recurrentes positivos, entonces la medida estacionaria $\pi$ existe y es única. Además:
\begin{equation*}
\pi_i = \frac{1}{m_i}, \quad i \in S
\end{equation*}
donde $m_i$ es el \textbf{tiempo medio de retorno} al estado $i$, es decir,
\begin{equation*}
m_i = E_i[T_i] \quad (\text{esperanza del tiempo hasta regresar a $i$ partiendo de $i$})
\end{equation*}
}

\textbf{Demostración:}

Para una cadena irreducible y recurrente positiva, consideremos la fracción de tiempo que la cadena pasa en cada estado. La fracción de tiempo en el estado $j$ hasta el tiempo $n$ es:
\begin{equation*}
\frac{1}{n} \sum_{k=1}^{n} \mathbf{1}_{\{X_k = j\}} \to \pi(j) \quad \text{cuando } n \to \infty
\end{equation*}

Por otro lado, si $N(j) = \sum_{k=1}^{n} \mathbf{1}_{\{X_k = j\}}$ es el número de visitas al estado $j$ hasta el tiempo $n$, entonces $\frac{n}{N(j)}$ converge al tiempo promedio entre visitas sucesivas a $j$, que es $m_j = E_j[T_j]$.

Por tanto:
\begin{equation*}
\lim_{n \to \infty} \frac{N(j)}{n} = \lim_{n \to \infty} \frac{1}{n/N(j)} = \frac{1}{m_j}
\end{equation*}

Definimos $\pi_j = \frac{1}{m_j}$. Para verificar que $\pi$ satisface $\pi P = \pi$:

Consideremos un ciclo de retorno al estado $i$. Durante este ciclo, el número esperado de visitas a cualquier estado $j$ es finito (pues la cadena es recurrente positiva). La suma de todas las visitas esperadas durante el ciclo debe ser igual a la longitud esperada del ciclo $m_i$.

Por la propiedad de Markov y la irreducibilidad, la proporción de tiempo en cada estado es independiente del estado inicial, lo que garantiza que $\pi$ es la única distribución que satisface $\pi P = \pi$ con $\sum_{j} \pi_j = 1$.

\qed

\teorema{Sea $X_n$ una cadena de Markov $\{X_n\}_{n \geq 0}$ cuyos estados son irreducibles; recurrentes positivos y aperiódicos. Entonces:
\begin{equation*}
\lim_{n \to \infty} P^n(x,y) = \pi(y)
\end{equation*}}

\textbf{Demostración}:

La demostración se basa en el análisis del comportamiento asintótico de las potencias de la matriz de transición. Procederemos en varios pasos.

\textbf{Paso 1: Existencia y unicidad de la distribución estacionaria}

Por ser la cadena irreducible y recurrente positiva, sabemos del teorema anterior que existe una única distribución estacionaria $\pi$ tal que $\pi P = \pi$ y $\sum_{y} \pi(y) = 1$.

\textbf{Paso 2: Uso de la aperiodicidad}

Como los estados son aperiódicos, para cada estado $x$ existe un entero $N_x$ tal que para todo $n \geq N_x$, se tiene $P^n(x,x) > 0$. Esto significa que es posible regresar al estado $x$ en cualquier número suficientemente grande de pasos.

\textbf{Paso 3: Acoplamiento y tiempo de mezcla}

Definimos el \emph{tiempo de acoplamiento} $\tau$ como el primer momento en que dos copias independientes de la cadena, iniciando desde estados diferentes, se encuentran en el mismo estado.

Para estados irreducibles, recurrentes positivos y aperiódicos, se puede demostrar que:
\begin{equation*}
\mathbb{E}[\tau] < \infty
\end{equation*}

\textbf{Paso 4: Convergencia en variación total}

Sea $\mu_n^{(x)}$ la distribución de $X_n$ dado $X_0 = x$. Entonces:
\begin{equation*}
\mu_n^{(x)}(y) = P^n(x,y)
\end{equation*}

La distancia en variación total entre $\mu_n^{(x)}$ y $\pi$ está dada por:
\begin{equation*}
\|\mu_n^{(x)} - \pi\|_{TV} = \frac{1}{2}\sum_{y \in S} |P^n(x,y) - \pi(y)|
\end{equation*}

\textbf{Paso 5: Demostración de la convergencia}

Usando la técnica de acoplamiento, se puede demostrar que existe una constante $\rho < 1$ tal que:
\begin{equation*}
\|\mu_n^{(x)} - \pi\|_{TV} \leq C \rho^n
\end{equation*}

para alguna constante $C > 0$. Esto implica convergencia exponencial.

En particular, para cada estado $y$:
\begin{equation*}
|P^n(x,y) - \pi(y)| \leq 2\|\mu_n^{(x)} - \pi\|_{TV} \leq 2C \rho^n \to 0 \text{ cuando } n \to \infty
\end{equation*}

Por lo tanto:
\begin{equation*}
\lim_{n \to \infty} P^n(x,y) = \pi(y)
\end{equation*}

\textbf{Paso 6: Interpretación del resultado}

Este teorema nos dice que, independientemente del estado inicial $x$, la probabilidad de estar en el estado $y$ después de $n$ pasos converge a $\pi(y)$ cuando $n \to \infty$. Esto significa que la cadena "olvida" su condición inicial y converge a su distribución de equilibrio.

La velocidad de convergencia es exponencial con tasa $\rho$, lo que hace que la convergencia sea relativamente rápida en la práctica.

\qed

\textbf{Corolario:} Si además la cadena es finita, entonces la convergencia es uniforme en el estado inicial:
\begin{equation*}
\lim_{n \to \infty} \max_{x,y \in S} |P^n(x,y) - \pi(y)| = 0
\end{equation*}

\section{Ejercicios}

\textbf{Problema:} Una molécula de hemoglobina puede transportar una molécula de oxígeno (+) o una molécula de monóxido de carbono (-).

Supongamos que los dos tipos de gases llegan con tasa 1 y 2, y el tiempo hasta conectar con tasa 3 y 4, respectivamente. Considerando el estado 0 cuando la molécula de hemoglobina está libre.

Determine en el largo plazo: la fracción de tiempo que la molécula de hemoglobina está en cada uno de sus tres posibles estados: $\{-, 0, +\}$.

\textbf{Sol:}

\textbf{Solución:}

Este es un problema de cadena de Markov continua que podemos modelar como un proceso de nacimiento y muerte.

\textbf{Estados:}
\begin{itemize}
\item Estado 0: Hemoglobina libre (sin gas)
\item Estado +: Hemoglobina con oxígeno
\item Estado -: Hemoglobina con monóxido de carbono
\end{itemize}

\textbf{Tasas de transición:}
\begin{itemize}
\item De estado 0 a estado +: tasa $\lambda_+ = 1$ (llegada de oxígeno)
\item De estado 0 a estado -: tasa $\lambda_- = 2$ (llegada de monóxido de carbono)
\item De estado + a estado 0: tasa $\mu_+ = 3$ (desconexión del oxígeno)
\item De estado - a estado 0: tasa $\mu_- = 4$ (desconexión del monóxido de carbono)
\end{itemize}

\textbf{Matriz generador infinitesimal:}
\begin{equation*}
Q = \begin{pmatrix}
-(1+2) & 1 & 2 \\
3 & -3 & 0 \\
4 & 0 & -4
\end{pmatrix} = \begin{pmatrix}
-3 & 1 & 2 \\
3 & -3 & 0 \\
4 & 0 & -4
\end{pmatrix}
\end{equation*}

\textbf{Ecuaciones de balance para la distribución estacionaria:}

Para encontrar la distribución estacionaria $\pi = (\pi_-, \pi_0, \pi_+)$, resolvemos $\pi Q = 0$ con $\pi_- + \pi_0 + \pi_+ = 1$.

Las ecuaciones de balance detallado son:
\begin{align}
-3\pi_0 + 3\pi_+ + 4\pi_- &= 0 \quad \text{(balance para estado 0)} \\
\pi_0 - 3\pi_+ &= 0 \quad \text{(balance para estado +)} \\
2\pi_0 - 4\pi_- &= 0 \quad \text{(balance para estado -)}
\end{align}

De la ecuación (2): $\pi_+ = \frac{\pi_0}{3}$

De la ecuación (3): $\pi_- = \frac{2\pi_0}{4} = \frac{\pi_0}{2}$

Sustituyendo en la condición de normalización:
\begin{equation*}
\pi_- + \pi_0 + \pi_+ = \frac{\pi_0}{2} + \pi_0 + \frac{\pi_0}{3} = 1
\end{equation*}

\begin{equation*}
\pi_0\left(\frac{1}{2} + 1 + \frac{1}{3}\right) = \pi_0\left(\frac{3 + 6 + 2}{6}\right) = \pi_0\left(\frac{11}{6}\right) = 1
\end{equation*}

Por lo tanto: $\pi_0 = \frac{6}{11}$

\textbf{Distribución estacionaria:}
\begin{align}
\pi_0 &= \frac{6}{11} \quad \text{(fracción de tiempo libre)} \\
\pi_+ &= \frac{1}{3} \cdot \frac{6}{11} = \frac{2}{11} \quad \text{(fracción de tiempo con oxígeno)} \\
\pi_- &= \frac{1}{2} \cdot \frac{6}{11} = \frac{3}{11} \quad \text{(fracción de tiempo con monóxido de carbono)}
\end{align}

\textbf{Verificación:} $\frac{6}{11} + \frac{2}{11} + \frac{3}{11} = \frac{11}{11} = 1$ (correcto)

\end{document}